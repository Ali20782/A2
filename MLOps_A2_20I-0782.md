# MLOps Assignment 2 Report | Ali Raza - 20I-0782

## Data Preprocessing Steps:

1.  Extracted data from https://www.dawn.com and https://www.bbc.com using BeautifulSoup
    
2.  Preprocessed text data by converting to lowercase, removing punctuation, extra whitespace, numbers, and common stopwords (a, and, then, etc.)
    

## DVC Setup:

1.  Initialized DVC in the data directory
    
2.  Added data to DVC
    
3.  Committed changes 
    
4.  Pushed changes to Github repo


## Data Pipeline Workflow

This workflow extracts data from (link unavailable) and (link unavailable), preprocesses the text data, and stores it on Google Drive. It also implements Data Version Control (DVC) to track versions of the data and pushes changes to a Github repo.

## Challenges Encountered:

The only problem encountered in this assignment was with setting up Apache Airflow. After a long time struggling I was able to find a solution for correctly setting up Airflow. Using a specific docker-compose file found by following the tutorial:
https://www.youtube.com/watch?v=Fl64Y0p7rls&list=LL&index=6&pp=gAQBiAQB  

Airflow was configured and running in Docker and I was able to access the DAG dashboard through localhost. However, challenges remained as when importing airflow into the jupyter notebook, I kept getting an error with the airflow formatter, which has not been resolved as of yet. 
    

## GitHub Repo:

The Github repo contains the jupyter file which contains all the code. It also includes a README file with descriptions of the assignment and the workflow.
